import os
import csv
import tempfile
from ..executor import SQLExecutor

def _build_insert_sql(table_name, fields, skip_duplicate=False):
    """构建插入SQL语句的公共方法"""
    field_names = ', '.join(fields)
    placeholders = '(' + ', '.join(['%s'] * len(fields)) + ')'
    insert_keyword = 'INSERT IGNORE' if skip_duplicate else 'INSERT'
    return f'{insert_keyword} INTO {table_name} ({field_names}) VALUES {placeholders}'


def insert(executor: SQLExecutor, table_name, insert_fields, skip_duplicate=False, commit=False, self_close=False, temp_dir=None):
    """
    智能SQL插入执行器方法，根据数据量自动选择最优插入策略
    
    策略选择：
    - 单条数据（dict）: 使用传统insert
    - 数据量 < 1000条: 使用现有executemany
    - 1000-50000条: 使用优化executemany（分批1000条）
    - 50000-100000条: 使用优化executemany（分批5000条）
    - 数据量 >= 100000条: 使用LOAD DATA INFILE（分批50000条）
    
    :param executor: SQLExecutor 实例
    :param table_name: 表名
    :param insert_fields: 字段和值，格式为字典或字典列表
    :param skip_duplicate: 是否跳过重复数据--基于主键或唯一索引(包含唯一复合索引)判断
        注意: 只有主键或当索引被明确设置为UNIQUE时才会触发跳过重复记录的行为,普通索引(如INDEX)不会导致跳过重复记录。
    :param commit: 是否自动提交
    :param self_close: 是否自动关闭连接
    :param temp_dir: 临时文件目录，默认为系统临时目录
    :return: 插入成功的记录数（int）
    """
    
    # 单条插入
    if isinstance(insert_fields, dict):
        sql = _build_insert_sql(table_name, insert_fields.keys(), skip_duplicate)
        values = tuple(insert_fields.values())
        executor.execute(sql, values, commit, self_close)
        return 1
        
    elif isinstance(insert_fields, list):
        if not insert_fields:
            if self_close:
                executor.close()
            return 0
            
        insert_num = len(insert_fields)
        
        # 根据数据量选择最优策略
        if insert_num < 1000:
            # 小数据量：使用现有方案
            sql = _build_insert_sql(table_name, insert_fields[0].keys(), skip_duplicate)
            values = [tuple(item.values()) for item in insert_fields]
            executor.execute(sql, values, commit, self_close)
            
        elif insert_num < 50000:
            # 中等数据量：优化executemany，分批1000条
            return _executemany_optimized(executor, table_name, insert_fields, skip_duplicate, commit, 1000, self_close)
            
        elif insert_num < 100000:
            # 大数据量：优化executemany，分批5000条
            return _executemany_optimized(executor, table_name, insert_fields, skip_duplicate, commit, 5000, self_close)
            
        else:
            # 超大数据量：使用LOAD DATA INFILE
            return _bulk_insert_load_data(executor, table_name, insert_fields, skip_duplicate, commit, 50000, temp_dir, self_close)
            
        return insert_num
    
    else:
        if self_close and commit :
            executor.close()
        raise ValueError("insert_fields must be a dict or a list of dicts")


def _bulk_insert_load_data(executor: SQLExecutor, table_name, insert_fields, skip_duplicate=False, 
                          commit=True, batch_size=50000, temp_dir=None, self_close=False):
    """
    使用LOAD DATA INFILE进行超高速批量插入，专为百万级数据量优化
    
    性能说明：
    - 160万条数据预计耗时30-60秒
    - 比传统executemany快20-50倍
    - 内存占用极低，支持流式处理
    """
    
    if not isinstance(insert_fields, list) or not insert_fields:
        if self_close:
            executor.close()
        return 0
    
    # 获取字段名和顺序
    field_names = list(insert_fields[0].keys())
    fields_str = ', '.join(field_names)
    
    total_records = len(insert_fields)
    inserted_count = 0
    total_batches = (total_records - 1) // batch_size + 1
    
    print(f"[LOAD DATA] Starting to process - total_records : {total_records} , total_batches : {total_batches}, batch_size : {batch_size} records")
    
    try:
        # 分批处理，避免单次文件过大
        for batch_start in range(0, total_records, batch_size):
            batch_end = min(batch_start + batch_size, total_records)
            batch_data = insert_fields[batch_start:batch_end]
            batch_num = batch_start // batch_size + 1
            
            print(f"[LOAD DATA] Processing batch {batch_num}/{total_batches} ({batch_start}-{batch_end-1})...")
            
            # 创建临时CSV文件
            with tempfile.NamedTemporaryFile(
                mode='w+', 
                suffix='.csv', 
                delete=False, 
                newline='', 
                dir=temp_dir,
                encoding='utf-8'
            ) as tmp_file:
                
                csv_writer = csv.writer(tmp_file, quoting=csv.QUOTE_MINIMAL)
                
                # 写入数据，确保字段顺序一致
                for row in batch_data:
                    csv_writer.writerow([str(row[field]) for field in field_names])
                
                tmp_file_path = tmp_file.name
            
            # 构造LOAD DATA语句
            load_sql = f"""
            LOAD DATA LOCAL INFILE '{tmp_file_path.replace(os.sep, '/')}'
            INTO TABLE {table_name}
            FIELDS TERMINATED BY ','
            OPTIONALLY ENCLOSED BY '"'
            LINES TERMINATED BY '\n'
            ({fields_str})
            """
            
            if skip_duplicate:
                load_sql += " IGNORE"
            
            # 执行批量插入 - executor.execute已处理异常和提交
            executor.execute(load_sql, commit=commit)
            inserted_count += len(batch_data)
            
            print(f"[LOAD DATA] Batch {batch_num}/{total_batches} completed, inserted {inserted_count}/{total_records} records")
            
            # 清理临时文件
            try:
                os.unlink(tmp_file_path)
            except OSError:
                pass  # 忽略文件删除错误
    
    finally:
        if self_close and commit :
            executor.close()
    
    print(f"[LOAD DATA] All completed! Total {inserted_count} records inserted")
    return inserted_count


def _executemany_optimized(executor: SQLExecutor, table_name, insert_fields, skip_duplicate=False, 
                          commit=True, batch_size=10000, self_close=False):
    """
    优化的分批executemany插入，适合1-50万数据量
    """
    
    if not isinstance(insert_fields, list) or not insert_fields:
        if self_close:
            executor.close()
        return 0
    
    total_records = len(insert_fields)
    sql = _build_insert_sql(table_name, insert_fields[0].keys(), skip_duplicate)
    inserted_count = 0
    total_batches = (total_records - 1) // batch_size + 1
    
    print(f"[executemany] Starting to process - total_records : {total_records} , total_batches : {total_batches}, batch_size : {batch_size} records")
    
    try:
        for batch_start in range(0, total_records, batch_size):
            batch_end = min(batch_start + batch_size, total_records)
            batch_data = insert_fields[batch_start:batch_end]
            batch_num = batch_start // batch_size + 1
            
            print(f"[executemany] Processing batch {batch_num}/{total_batches} ({batch_start}-{batch_end-1})...")
            
            values = [tuple(item.values()) for item in batch_data]
            
            # 执行批量插入 - executor.execute已处理异常和提交
            executor.execute(sql, values, commit=commit)
            inserted_count += len(batch_data)
            
            print(f"[executemany] Batch {batch_num}/{total_batches} completed, inserted {inserted_count}/{total_records} records")
    
    finally:
        if self_close and commit :
            executor.close()
    
    print(f"[executemany] All completed! Total {inserted_count} records inserted")
    return inserted_count